{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Library","metadata":{}},{"cell_type":"code","source":"!pip install datasets==2.1.0\n!pip install transformers==4.18.0\n\nimport os\nimport json\nimport time\nfrom tqdm import tqdm\nimport glob\nfrom IPython.display import FileLink\n\n\nimport pandas as pd\nimport numpy as np\nfrom collections import Counter\n\nfrom matplotlib import pyplot as plt\n\n\nimport torchaudio\nfrom datasets import load_dataset, load_metric, load_from_disk\nfrom transformers import AutoFeatureExtractor, AutoModelForAudioClassification, TrainingArguments, Trainer","metadata":{"execution":{"iopub.status.busy":"2022-12-22T22:01:00.963918Z","iopub.execute_input":"2022-12-22T22:01:00.964800Z","iopub.status.idle":"2022-12-22T22:01:36.487675Z","shell.execute_reply.started":"2022-12-22T22:01:00.964699Z","shell.execute_reply":"2022-12-22T22:01:36.486701Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets==2.1.0 in /opt/conda/lib/python3.7/site-packages (2.1.0)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.7/site-packages (from datasets==2.1.0) (0.70.13)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.7/site-packages (from datasets==2.1.0) (0.18.0)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.7/site-packages (from datasets==2.1.0) (5.0.0)\nRequirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from datasets==2.1.0) (0.3.5.1)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from datasets==2.1.0) (21.3)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from datasets==2.1.0) (4.13.0)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from datasets==2.1.0) (1.3.5)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.7/site-packages (from datasets==2.1.0) (2022.8.2)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.7/site-packages (from datasets==2.1.0) (4.64.0)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from datasets==2.1.0) (2.28.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from datasets==2.1.0) (1.21.6)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.7/site-packages (from datasets==2.1.0) (3.8.1)\nRequirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from datasets==2.1.0) (0.10.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.7/site-packages (from datasets==2.1.0) (3.0.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets==2.1.0) (1.3.0)\nRequirement already satisfied: typing-extensions>=3.7.4 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets==2.1.0) (4.1.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets==2.1.0) (1.2.0)\nRequirement already satisfied: asynctest==0.13.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets==2.1.0) (0.13.0)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets==2.1.0) (4.0.2)\nRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets==2.1.0) (2.1.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets==2.1.0) (6.0.2)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets==2.1.0) (21.4.0)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets==2.1.0) (1.7.2)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets==2.1.0) (6.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets==2.1.0) (3.7.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->datasets==2.1.0) (3.0.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets==2.1.0) (3.3)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets==2.1.0) (2022.9.24)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets==2.1.0) (1.26.12)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->datasets==2.1.0) (3.8.0)\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets==2.1.0) (2.8.2)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets==2.1.0) (2022.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->datasets==2.1.0) (1.15.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting transformers==4.18.0\n  Downloading transformers-4.18.0-py3-none-any.whl (4.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers==4.18.0) (1.21.6)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers==4.18.0) (4.64.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers==4.18.0) (3.7.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers==4.18.0) (6.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers==4.18.0) (2.28.1)\nRequirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers==4.18.0) (0.12.1)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers==4.18.0) (21.3)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers==4.18.0) (2021.11.10)\nRequirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers==4.18.0) (0.0.53)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers==4.18.0) (4.13.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from transformers==4.18.0) (0.10.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.18.0) (4.1.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->transformers==4.18.0) (3.0.9)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers==4.18.0) (3.8.0)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.18.0) (2.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.18.0) (3.3)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.18.0) (1.26.12)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.18.0) (2022.9.24)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.18.0) (1.15.0)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.18.0) (1.0.1)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.18.0) (8.0.4)\nInstalling collected packages: transformers\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.20.1\n    Uninstalling transformers-4.20.1:\n      Successfully uninstalled transformers-4.20.1\nSuccessfully installed transformers-4.18.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"markdown","source":"# Read dataset","metadata":{}},{"cell_type":"code","source":"paths = []\ncategories = []\nfor i in glob.glob(\"/kaggle/input/outube-dataset-1800/youtube_dataset_1800/fold1/*\"):\n    if os.path.getsize(i) / 1024 / 1024 < 100:\n        categories += [0]\n        paths += [i]\nfor i in glob.glob(\"/kaggle/input/outube-dataset-1800/youtube_dataset_1800/fold2/*\"):\n    if os.path.getsize(i) / 1024 / 1024 < 100:\n        categories += [1]\n        paths += [i]\nfor i in glob.glob(\"/kaggle/input/outube-dataset-1800/youtube_dataset_1800/fold3/*\"):\n    if os.path.getsize(i) / 1024 / 1024 < 100:\n        categories += [2]\n        paths += [i]\nfor i in glob.glob(\"/kaggle/input/outube-dataset-1800/youtube_dataset_1800/fold4/*\"):\n    if os.path.getsize(i) / 1024 / 1024 < 100:\n        categories += [3]\n        paths += [i]","metadata":{"execution":{"iopub.status.busy":"2022-12-22T19:35:46.278294Z","iopub.execute_input":"2022-12-22T19:35:46.278722Z","iopub.status.idle":"2022-12-22T19:35:46.589537Z","shell.execute_reply.started":"2022-12-22T19:35:46.278691Z","shell.execute_reply":"2022-12-22T19:35:46.588368Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"ct = Counter(categories)\nplt.bar([x for x in ct.keys()], ct.values())\nplt.show()\nCounter(categories)","metadata":{"execution":{"iopub.status.busy":"2022-12-22T19:35:13.525459Z","iopub.execute_input":"2022-12-22T19:35:13.525888Z","iopub.status.idle":"2022-12-22T19:35:13.736492Z","shell.execute_reply.started":"2022-12-22T19:35:13.525852Z","shell.execute_reply":"2022-12-22T19:35:13.735252Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"Counter({0: 109, 1: 109, 2: 109, 3: 109})"},"metadata":{}}]},{"cell_type":"code","source":"model_checkpoint = \"facebook/wav2vec2-base\"\nmax_duration = 60","metadata":{"execution":{"iopub.status.busy":"2022-12-22T19:36:05.859445Z","iopub.execute_input":"2022-12-22T19:36:05.859854Z","iopub.status.idle":"2022-12-22T19:36:05.865909Z","shell.execute_reply.started":"2022-12-22T19:36:05.859822Z","shell.execute_reply":"2022-12-22T19:36:05.864595Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"arraies = []\nlabels = []\nsampling_rates = []\n\nn_c = {\"history\": 0, \"art_music\":1, \"travel\":2, \"food\":3}\n\nfor i in tqdm(range(len(categories))):\n    try:\n        arr, org_sr = torchaudio.load(paths[i])\n        arr = torchaudio.functional.resample(arr, orig_freq=org_sr, new_freq=16000)[:, :org_sr * max_duration]\n        if len(arr[0]) == 2:\n            arraies += [np.array((arr[0] + arr[1])/2)]\n        else:\n            arraies += [np.array(arr[0])]\n        sampling_rates += [org_sr]\n        #labels +=  [n_c[categories[i]]]\n        labels +=  [categories[i]]\n    except:\n        print(i)","metadata":{"execution":{"iopub.status.busy":"2022-12-22T19:36:24.453570Z","iopub.execute_input":"2022-12-22T19:36:24.454085Z","iopub.status.idle":"2022-12-22T20:04:07.869229Z","shell.execute_reply.started":"2022-12-22T19:36:24.454042Z","shell.execute_reply":"2022-12-22T20:04:07.867303Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"100%|██████████| 544/544 [27:43<00:00,  3.06s/it]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Make dataset","metadata":{}},{"cell_type":"code","source":"df_train_arraies = []\ndf_valid_arraies = []\ndf_train_labels = []\ndf_valid_labels = []\nlimit = 14\nfor i in range(len(labels)):\n    if df_valid_labels.count(labels[i]) < limit:\n        df_valid_labels += [labels[i]]\n        df_valid_arraies +=[arraies[i]]\n    else:\n        df_train_labels += [labels[i]]\n        df_train_arraies += [arraies[i]] ","metadata":{"execution":{"iopub.status.busy":"2022-12-22T20:09:16.378499Z","iopub.execute_input":"2022-12-22T20:09:16.378937Z","iopub.status.idle":"2022-12-22T20:09:16.386927Z","shell.execute_reply.started":"2022-12-22T20:09:16.378907Z","shell.execute_reply":"2022-12-22T20:09:16.385596Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"from datasets import Dataset, DatasetDict\n\ntdf =pd.DataFrame({\"array\": df_train_arraies, \"sampling_rate\": [0 for i in df_train_arraies ] , \"label\": df_train_labels })\nvdf =pd.DataFrame({\"array\": df_valid_arraies, \"sampling_rate\": [0 for i in df_valid_arraies ], \"label\": df_valid_labels })\ntdf = tdf.sample(frac=1).reset_index(drop=True)\n\ntds = Dataset.from_pandas(tdf)\nvds = Dataset.from_pandas(vdf)\n\nds = DatasetDict()\nds[\"train\"] =  tds\nds[\"validation\"] =  vds","metadata":{"execution":{"iopub.status.busy":"2022-12-22T20:09:20.968923Z","iopub.execute_input":"2022-12-22T20:09:20.969377Z","iopub.status.idle":"2022-12-22T20:09:51.317282Z","shell.execute_reply.started":"2022-12-22T20:09:20.969321Z","shell.execute_reply":"2022-12-22T20:09:51.316004Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"'''ds.save_to_disk(\"dataset_1800\")              #  Saave reserve copy of dataset\n!tar -cvf archive_1800.tar.gz /kaggle/working/dataset_1800\nFileLink(r'archive_1800.tar.gz')\n\nds = load_from_disk(\"/kaggle/working/dataset_1800_30\")\n!cp -r /kaggle/input/archive-1800-30/kaggle/working/dataset_1800_30 ./'''","metadata":{"execution":{"iopub.status.busy":"2022-12-22T20:10:17.405362Z","iopub.execute_input":"2022-12-22T20:10:17.406301Z","iopub.status.idle":"2022-12-22T20:10:26.316512Z","shell.execute_reply.started":"2022-12-22T20:10:17.406261Z","shell.execute_reply":"2022-12-22T20:10:26.315566Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"# Feature extractor","metadata":{}},{"cell_type":"code","source":"feature_extractor = AutoFeatureExtractor.from_pretrained(\"facebook/wav2vec2-base\")\nfeature_extractor","metadata":{"execution":{"iopub.status.busy":"2022-12-22T22:02:48.887903Z","iopub.execute_input":"2022-12-22T22:02:48.888281Z","iopub.status.idle":"2022-12-22T22:02:50.312811Z","shell.execute_reply.started":"2022-12-22T22:02:48.888250Z","shell.execute_reply":"2022-12-22T22:02:50.311834Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/159 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f05292f06a0347e5bea231d3adb5679a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.80k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e91f16b853884284859b5907690804e4"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/transformers/configuration_utils.py:359: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n  \"Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 \"\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"Wav2Vec2FeatureExtractor {\n  \"do_normalize\": true,\n  \"feature_extractor_type\": \"Wav2Vec2FeatureExtractor\",\n  \"feature_size\": 1,\n  \"padding_side\": \"right\",\n  \"padding_value\": 0.0,\n  \"return_attention_mask\": false,\n  \"sampling_rate\": 16000\n}"},"metadata":{}}]},{"cell_type":"code","source":"def preprocess_function(examples):\n    audio_arrays = examples[\"array\"]\n    inputs = feature_extractor(\n        audio_arrays, \n        sampling_rate=feature_extractor.sampling_rate, \n        max_length=int(feature_extractor.sampling_rate * max_duration), \n        truncation=True,\n        padding=True\n    )\n    return inputs","metadata":{"execution":{"iopub.status.busy":"2022-12-22T22:02:50.759772Z","iopub.execute_input":"2022-12-22T22:02:50.760447Z","iopub.status.idle":"2022-12-22T22:02:50.766544Z","shell.execute_reply.started":"2022-12-22T22:02:50.760408Z","shell.execute_reply":"2022-12-22T22:02:50.765314Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"encoded_dataset = ds.map(preprocess_function, remove_columns=[\"sampling_rate\", \"array\"], batched=True, batch_size=2)\nencoded_dataset","metadata":{"execution":{"iopub.status.busy":"2022-12-22T22:02:54.160946Z","iopub.execute_input":"2022-12-22T22:02:54.161345Z","iopub.status.idle":"2022-12-22T22:06:42.554850Z","shell.execute_reply.started":"2022-12-22T22:02:54.161312Z","shell.execute_reply":"2022-12-22T22:06:42.554179Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/196 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b96f91ed9de429baa2f0bbe8285defc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/22 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cbd2bbbe3dce448fbd28ccf1baad4aaa"}},"metadata":{}},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['label', 'input_values'],\n        num_rows: 392\n    })\n    validation: Dataset({\n        features: ['label', 'input_values'],\n        num_rows: 44\n    })\n})"},"metadata":{}}]},{"cell_type":"markdown","source":"# Load model","metadata":{}},{"cell_type":"code","source":"num_labels = 4\nmodel = AutoModelForAudioClassification.from_pretrained(\n    \"facebook/wav2vec2-base\", \n    num_labels=num_labels\n)","metadata":{"execution":{"iopub.status.busy":"2022-12-22T22:07:12.794361Z","iopub.execute_input":"2022-12-22T22:07:12.795313Z","iopub.status.idle":"2022-12-22T22:07:22.942916Z","shell.execute_reply.started":"2022-12-22T22:07:12.795265Z","shell.execute_reply":"2022-12-22T22:07:22.941921Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/363M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c13a7f28f28d470e8a0f07900b160099"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2ForSequenceClassification: ['quantizer.weight_proj.bias', 'project_q.weight', 'project_q.bias', 'project_hid.bias', 'quantizer.codevectors', 'project_hid.weight', 'quantizer.weight_proj.weight']\n- This IS expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['projector.bias', 'projector.weight', 'classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"model_name = \"facebook/wav2vec2-base\".split(\"/\")[-1]\nargs = TrainingArguments(\n    f\"{model_name}-finetuned-ks\",\n    evaluation_strategy = \"epoch\",\n    save_total_limit = 3,\n    save_strategy = \"epoch\",\n    learning_rate=1e-5,\n    per_device_train_batch_size=8,\n    gradient_accumulation_steps=4,\n    per_device_eval_batch_size=8,\n    num_train_epochs=45,\n    warmup_ratio=0.1,\n    logging_steps=1,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"accuracy\",\n)","metadata":{"execution":{"iopub.status.busy":"2022-12-22T22:07:22.945069Z","iopub.execute_input":"2022-12-22T22:07:22.945472Z","iopub.status.idle":"2022-12-22T22:07:23.041313Z","shell.execute_reply.started":"2022-12-22T22:07:22.945432Z","shell.execute_reply":"2022-12-22T22:07:23.040302Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"metric = load_metric(\"accuracy\")\n\ndef compute_metrics(eval_pred):\n    \"\"\"Computes accuracy on a batch of predictions\"\"\"\n    predictions = np.argmax(eval_pred.predictions, axis=1)\n    print(predictions)\n    print(eval_pred.label_ids)\n    print(metric.compute(predictions=predictions, references=eval_pred.label_ids))\n    return metric.compute(predictions=predictions, references=eval_pred.label_ids)","metadata":{"execution":{"iopub.status.busy":"2022-12-22T22:07:23.230221Z","iopub.execute_input":"2022-12-22T22:07:23.233040Z","iopub.status.idle":"2022-12-22T22:07:24.175228Z","shell.execute_reply.started":"2022-12-22T22:07:23.232987Z","shell.execute_reply":"2022-12-22T22:07:24.174330Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/1.41k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed1bb92921394a7a840f99395adc378a"}},"metadata":{}}]},{"cell_type":"markdown","source":"# Train","metadata":{}},{"cell_type":"code","source":"trainer = Trainer(\n    model,\n    args,\n    train_dataset=encoded_dataset[\"train\"],\n    eval_dataset=encoded_dataset[\"validation\"],\n    tokenizer=feature_extractor,\n    compute_metrics=compute_metrics\n)","metadata":{"execution":{"iopub.status.busy":"2022-12-22T22:07:24.176943Z","iopub.execute_input":"2022-12-22T22:07:24.177367Z","iopub.status.idle":"2022-12-22T22:07:30.255217Z","shell.execute_reply.started":"2022-12-22T22:07:24.177330Z","shell.execute_reply":"2022-12-22T22:07:30.254058Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2022-12-22T22:07:30.257285Z","iopub.execute_input":"2022-12-22T22:07:30.257662Z","iopub.status.idle":"2022-12-23T01:07:04.011485Z","shell.execute_reply.started":"2022-12-22T22:07:30.257617Z","shell.execute_reply":"2022-12-23T01:07:04.009964Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 392\n  Num Epochs = 45\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 32\n  Gradient Accumulation steps = 4\n  Total optimization steps = 540\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.13.7 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.12.21"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20221222_220755-x12p3laq</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href=\"https://wandb.ai/nikitaromanoov/huggingface/runs/x12p3laq\" target=\"_blank\">wav2vec2-base-finetuned-ks</a></strong> to <a href=\"https://wandb.ai/nikitaromanoov/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='540' max='540' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [540/540 2:58:31, Epoch 44/45]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>1.391500</td>\n      <td>1.385867</td>\n      <td>0.227273</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>1.371700</td>\n      <td>1.371648</td>\n      <td>0.295455</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.333100</td>\n      <td>1.347392</td>\n      <td>0.431818</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.308400</td>\n      <td>1.312891</td>\n      <td>0.431818</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>1.274800</td>\n      <td>1.265239</td>\n      <td>0.454545</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>1.212600</td>\n      <td>1.239604</td>\n      <td>0.477273</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>1.238500</td>\n      <td>1.204087</td>\n      <td>0.454545</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>1.133600</td>\n      <td>1.183821</td>\n      <td>0.454545</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>1.157400</td>\n      <td>1.165847</td>\n      <td>0.454545</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>1.023800</td>\n      <td>1.182422</td>\n      <td>0.454545</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>1.049700</td>\n      <td>1.186563</td>\n      <td>0.522727</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>0.942000</td>\n      <td>1.119242</td>\n      <td>0.568182</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>0.945300</td>\n      <td>1.108770</td>\n      <td>0.590909</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>0.942900</td>\n      <td>1.108791</td>\n      <td>0.545455</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>1.034200</td>\n      <td>1.092893</td>\n      <td>0.568182</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>0.759300</td>\n      <td>1.076075</td>\n      <td>0.590909</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>0.718500</td>\n      <td>1.037025</td>\n      <td>0.590909</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>0.871200</td>\n      <td>1.098389</td>\n      <td>0.568182</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>0.826000</td>\n      <td>1.028302</td>\n      <td>0.613636</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>0.635800</td>\n      <td>1.074962</td>\n      <td>0.545455</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.606100</td>\n      <td>1.070291</td>\n      <td>0.590909</td>\n    </tr>\n    <tr>\n      <td>21</td>\n      <td>0.586900</td>\n      <td>1.093332</td>\n      <td>0.500000</td>\n    </tr>\n    <tr>\n      <td>22</td>\n      <td>0.620500</td>\n      <td>1.050306</td>\n      <td>0.590909</td>\n    </tr>\n    <tr>\n      <td>23</td>\n      <td>0.456900</td>\n      <td>1.073032</td>\n      <td>0.590909</td>\n    </tr>\n    <tr>\n      <td>24</td>\n      <td>0.500800</td>\n      <td>1.035177</td>\n      <td>0.568182</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>0.420800</td>\n      <td>1.184530</td>\n      <td>0.545455</td>\n    </tr>\n    <tr>\n      <td>26</td>\n      <td>0.507400</td>\n      <td>1.042715</td>\n      <td>0.613636</td>\n    </tr>\n    <tr>\n      <td>27</td>\n      <td>0.437800</td>\n      <td>1.127726</td>\n      <td>0.522727</td>\n    </tr>\n    <tr>\n      <td>28</td>\n      <td>0.347900</td>\n      <td>1.093317</td>\n      <td>0.568182</td>\n    </tr>\n    <tr>\n      <td>29</td>\n      <td>0.365900</td>\n      <td>1.070666</td>\n      <td>0.590909</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.463200</td>\n      <td>1.139000</td>\n      <td>0.545455</td>\n    </tr>\n    <tr>\n      <td>31</td>\n      <td>0.451500</td>\n      <td>1.081343</td>\n      <td>0.568182</td>\n    </tr>\n    <tr>\n      <td>32</td>\n      <td>0.288700</td>\n      <td>1.061115</td>\n      <td>0.568182</td>\n    </tr>\n    <tr>\n      <td>33</td>\n      <td>0.273400</td>\n      <td>1.036167</td>\n      <td>0.590909</td>\n    </tr>\n    <tr>\n      <td>34</td>\n      <td>0.280000</td>\n      <td>1.060686</td>\n      <td>0.613636</td>\n    </tr>\n    <tr>\n      <td>35</td>\n      <td>0.244200</td>\n      <td>1.083112</td>\n      <td>0.613636</td>\n    </tr>\n    <tr>\n      <td>36</td>\n      <td>0.218800</td>\n      <td>1.069467</td>\n      <td>0.636364</td>\n    </tr>\n    <tr>\n      <td>37</td>\n      <td>0.344200</td>\n      <td>1.087904</td>\n      <td>0.590909</td>\n    </tr>\n    <tr>\n      <td>38</td>\n      <td>0.266000</td>\n      <td>1.104827</td>\n      <td>0.590909</td>\n    </tr>\n    <tr>\n      <td>39</td>\n      <td>0.221200</td>\n      <td>1.096281</td>\n      <td>0.636364</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.211400</td>\n      <td>1.102390</td>\n      <td>0.613636</td>\n    </tr>\n    <tr>\n      <td>41</td>\n      <td>0.201200</td>\n      <td>1.100977</td>\n      <td>0.636364</td>\n    </tr>\n    <tr>\n      <td>42</td>\n      <td>0.211600</td>\n      <td>1.114125</td>\n      <td>0.613636</td>\n    </tr>\n    <tr>\n      <td>43</td>\n      <td>0.193200</td>\n      <td>1.110731</td>\n      <td>0.590909</td>\n    </tr>\n    <tr>\n      <td>44</td>\n      <td>0.208200</td>\n      <td>1.112909</td>\n      <td>0.613636</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"***** Running Evaluation *****\n  Num examples = 44\n  Batch size = 8\n","output_type":"stream"},{"name":"stdout","text":"[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2\n 2 2 2 1 2 2 2]\n[0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3\n 3 3 3 3 3 3 3]\n{'accuracy': 0.22727272727272727}\n","output_type":"stream"},{"name":"stderr","text":"Saving model checkpoint to wav2vec2-base-finetuned-ks/checkpoint-12\nConfiguration saved in wav2vec2-base-finetuned-ks/checkpoint-12/config.json\nModel weights saved in wav2vec2-base-finetuned-ks/checkpoint-12/pytorch_model.bin\nFeature extractor saved in wav2vec2-base-finetuned-ks/checkpoint-12/preprocessor_config.json\n***** Running Evaluation *****\n  Num examples = 44\n  Batch size = 8\n","output_type":"stream"},{"name":"stdout","text":"[2 2 2 2 2 2 2 2 2 0 2 1 1 2 2 2 1 2 1 1 2 1 2 1 2 1 1 2 1 2 1 2 1 3 2 2 2\n 2 2 2 1 1 1 1]\n[0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3\n 3 3 3 3 3 3 3]\n{'accuracy': 0.29545454545454547}\n","output_type":"stream"},{"name":"stderr","text":"Saving model checkpoint to wav2vec2-base-finetuned-ks/checkpoint-24\nConfiguration saved in wav2vec2-base-finetuned-ks/checkpoint-24/config.json\nModel weights saved in wav2vec2-base-finetuned-ks/checkpoint-24/pytorch_model.bin\nFeature extractor saved in wav2vec2-base-finetuned-ks/checkpoint-24/preprocessor_config.json\n***** Running Evaluation *****\n  Num examples = 44\n  Batch size = 8\n","output_type":"stream"},{"name":"stdout","text":"[0 2 0 2 0 2 2 0 0 0 0 1 1 2 2 2 1 1 1 1 2 1 2 1 1 1 1 2 1 2 1 2 1 3 2 1 0\n 2 2 2 1 1 1 1]\n[0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3\n 3 3 3 3 3 3 3]\n{'accuracy': 0.4318181818181818}\n","output_type":"stream"},{"name":"stderr","text":"Saving model checkpoint to wav2vec2-base-finetuned-ks/checkpoint-36\nConfiguration saved in wav2vec2-base-finetuned-ks/checkpoint-36/config.json\nModel weights saved in wav2vec2-base-finetuned-ks/checkpoint-36/pytorch_model.bin\nFeature extractor saved in wav2vec2-base-finetuned-ks/checkpoint-36/preprocessor_config.json\n***** Running Evaluation *****\n  Num examples = 44\n  Batch size = 8\n","output_type":"stream"},{"name":"stdout","text":"[0 2 0 2 0 0 2 0 0 0 0 1 1 0 2 1 1 1 1 1 2 1 2 1 1 1 1 2 1 3 1 1 1 3 2 1 0\n 2 2 2 1 1 1 1]\n[0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3\n 3 3 3 3 3 3 3]\n{'accuracy': 0.4318181818181818}\n","output_type":"stream"},{"name":"stderr","text":"Saving model checkpoint to wav2vec2-base-finetuned-ks/checkpoint-48\nConfiguration saved in wav2vec2-base-finetuned-ks/checkpoint-48/config.json\nModel weights saved in wav2vec2-base-finetuned-ks/checkpoint-48/pytorch_model.bin\nFeature extractor saved in wav2vec2-base-finetuned-ks/checkpoint-48/preprocessor_config.json\nDeleting older checkpoint [wav2vec2-base-finetuned-ks/checkpoint-12] due to args.save_total_limit\n***** Running Evaluation *****\n  Num examples = 44\n  Batch size = 8\n","output_type":"stream"},{"name":"stdout","text":"[0 2 0 3 0 0 0 0 0 0 0 1 1 0 0 1 1 1 1 1 0 1 2 1 1 1 1 0 1 3 1 1 1 3 3 1 0\n 0 0 0 1 1 1 1]\n[0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3\n 3 3 3 3 3 3 3]\n{'accuracy': 0.45454545454545453}\n","output_type":"stream"},{"name":"stderr","text":"Saving model checkpoint to wav2vec2-base-finetuned-ks/checkpoint-60\nConfiguration saved in wav2vec2-base-finetuned-ks/checkpoint-60/config.json\nModel weights saved in wav2vec2-base-finetuned-ks/checkpoint-60/pytorch_model.bin\nFeature extractor saved in wav2vec2-base-finetuned-ks/checkpoint-60/preprocessor_config.json\nDeleting older checkpoint [wav2vec2-base-finetuned-ks/checkpoint-24] due to args.save_total_limit\n***** Running Evaluation *****\n  Num examples = 44\n  Batch size = 8\n","output_type":"stream"},{"name":"stdout","text":"[0 0 0 3 0 0 0 0 0 0 0 1 1 0 0 1 1 1 1 0 0 1 0 1 3 1 3 0 1 3 1 3 3 3 3 1 0\n 0 0 0 1 1 3 3]\n[0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3\n 3 3 3 3 3 3 3]\n{'accuracy': 0.4772727272727273}\n","output_type":"stream"},{"name":"stderr","text":"Saving model checkpoint to wav2vec2-base-finetuned-ks/checkpoint-72\nConfiguration saved in wav2vec2-base-finetuned-ks/checkpoint-72/config.json\nModel weights saved in wav2vec2-base-finetuned-ks/checkpoint-72/pytorch_model.bin\nFeature extractor saved in wav2vec2-base-finetuned-ks/checkpoint-72/preprocessor_config.json\nDeleting older checkpoint [wav2vec2-base-finetuned-ks/checkpoint-36] due to args.save_total_limit\n***** Running Evaluation *****\n  Num examples = 44\n  Batch size = 8\n","output_type":"stream"},{"name":"stdout","text":"[0 2 0 0 0 0 0 0 0 0 0 1 1 0 0 1 1 1 0 1 0 1 0 1 3 0 1 0 1 3 0 3 3 1 3 1 0\n 0 0 0 3 1 3 1]\n[0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3\n 3 3 3 3 3 3 3]\n{'accuracy': 0.45454545454545453}\n","output_type":"stream"},{"name":"stderr","text":"Saving model checkpoint to wav2vec2-base-finetuned-ks/checkpoint-84\nConfiguration saved in wav2vec2-base-finetuned-ks/checkpoint-84/config.json\nModel weights saved in wav2vec2-base-finetuned-ks/checkpoint-84/pytorch_model.bin\nFeature extractor saved in wav2vec2-base-finetuned-ks/checkpoint-84/preprocessor_config.json\nDeleting older checkpoint [wav2vec2-base-finetuned-ks/checkpoint-48] due to args.save_total_limit\n***** Running Evaluation *****\n  Num examples = 44\n  Batch size = 8\n","output_type":"stream"},{"name":"stdout","text":"[0 2 0 3 0 0 0 0 0 0 0 1 1 0 0 1 1 1 2 1 3 1 0 1 2 0 0 3 1 3 1 3 3 1 3 1 0\n 0 0 0 3 1 3 1]\n[0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3\n 3 3 3 3 3 3 3]\n{'accuracy': 0.45454545454545453}\n","output_type":"stream"},{"name":"stderr","text":"Saving model checkpoint to wav2vec2-base-finetuned-ks/checkpoint-96\nConfiguration saved in wav2vec2-base-finetuned-ks/checkpoint-96/config.json\nModel weights saved in wav2vec2-base-finetuned-ks/checkpoint-96/pytorch_model.bin\nFeature extractor saved in wav2vec2-base-finetuned-ks/checkpoint-96/preprocessor_config.json\nDeleting older checkpoint [wav2vec2-base-finetuned-ks/checkpoint-60] due to args.save_total_limit\n***** Running Evaluation *****\n  Num examples = 44\n  Batch size = 8\n","output_type":"stream"},{"name":"stdout","text":"[0 2 0 0 0 0 0 0 0 0 0 1 1 0 0 1 1 1 0 0 3 1 0 1 3 1 1 0 1 3 1 3 3 1 3 1 0\n 0 0 0 3 1 3 3]\n[0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3\n 3 3 3 3 3 3 3]\n{'accuracy': 0.45454545454545453}\n","output_type":"stream"},{"name":"stderr","text":"Saving model checkpoint to wav2vec2-base-finetuned-ks/checkpoint-108\nConfiguration saved in wav2vec2-base-finetuned-ks/checkpoint-108/config.json\nModel weights saved in wav2vec2-base-finetuned-ks/checkpoint-108/pytorch_model.bin\nFeature extractor saved in wav2vec2-base-finetuned-ks/checkpoint-108/preprocessor_config.json\nDeleting older checkpoint [wav2vec2-base-finetuned-ks/checkpoint-84] due to args.save_total_limit\n***** Running Evaluation *****\n  Num examples = 44\n  Batch size = 8\n","output_type":"stream"},{"name":"stdout","text":"[0 2 0 3 0 0 0 0 0 0 0 1 0 0 0 1 1 1 3 0 3 1 3 2 3 0 0 3 1 3 0 3 3 3 3 2 0\n 3 0 0 3 0 3 1]\n[0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3\n 3 3 3 3 3 3 3]\n{'accuracy': 0.45454545454545453}\n","output_type":"stream"},{"name":"stderr","text":"Saving model checkpoint to wav2vec2-base-finetuned-ks/checkpoint-120\nConfiguration saved in wav2vec2-base-finetuned-ks/checkpoint-120/config.json\nModel weights saved in wav2vec2-base-finetuned-ks/checkpoint-120/pytorch_model.bin\nFeature extractor saved in wav2vec2-base-finetuned-ks/checkpoint-120/preprocessor_config.json\nDeleting older checkpoint [wav2vec2-base-finetuned-ks/checkpoint-96] due to args.save_total_limit\n***** Running Evaluation *****\n  Num examples = 44\n  Batch size = 8\n","output_type":"stream"},{"name":"stdout","text":"[0 2 0 0 0 0 0 0 0 0 0 1 1 0 0 1 1 1 0 0 3 1 0 2 3 0 0 0 1 3 0 2 2 3 3 2 0\n 0 0 0 3 1 3 2]\n[0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3\n 3 3 3 3 3 3 3]\n{'accuracy': 0.5227272727272727}\n","output_type":"stream"},{"name":"stderr","text":"Saving model checkpoint to wav2vec2-base-finetuned-ks/checkpoint-132\nConfiguration saved in wav2vec2-base-finetuned-ks/checkpoint-132/config.json\nModel weights saved in wav2vec2-base-finetuned-ks/checkpoint-132/pytorch_model.bin\nFeature extractor saved in wav2vec2-base-finetuned-ks/checkpoint-132/preprocessor_config.json\nDeleting older checkpoint [wav2vec2-base-finetuned-ks/checkpoint-72] due to args.save_total_limit\n***** Running Evaluation *****\n  Num examples = 44\n  Batch size = 8\n","output_type":"stream"},{"name":"stdout","text":"[0 2 0 3 0 0 0 0 0 0 0 1 1 0 0 1 1 1 2 1 3 1 2 2 3 0 0 3 1 3 1 3 2 3 3 3 0\n 3 0 0 3 1 3 1]\n[0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3\n 3 3 3 3 3 3 3]\n{'accuracy': 0.5681818181818182}\n","output_type":"stream"},{"name":"stderr","text":"Saving model checkpoint to wav2vec2-base-finetuned-ks/checkpoint-144\nConfiguration saved in wav2vec2-base-finetuned-ks/checkpoint-144/config.json\nModel weights saved in wav2vec2-base-finetuned-ks/checkpoint-144/pytorch_model.bin\nFeature extractor saved in wav2vec2-base-finetuned-ks/checkpoint-144/preprocessor_config.json\nDeleting older checkpoint [wav2vec2-base-finetuned-ks/checkpoint-108] due to args.save_total_limit\n***** Running Evaluation *****\n  Num examples = 44\n  Batch size = 8\n","output_type":"stream"},{"name":"stdout","text":"[0 2 0 0 0 0 0 0 0 0 0 1 1 0 0 1 1 1 0 0 3 1 2 2 3 0 0 0 1 3 1 2 2 3 3 3 0\n 3 0 0 3 1 3 1]\n[0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3\n 3 3 3 3 3 3 3]\n{'accuracy': 0.5909090909090909}\n","output_type":"stream"},{"name":"stderr","text":"Saving model checkpoint to wav2vec2-base-finetuned-ks/checkpoint-156\nConfiguration saved in wav2vec2-base-finetuned-ks/checkpoint-156/config.json\nModel weights saved in wav2vec2-base-finetuned-ks/checkpoint-156/pytorch_model.bin\nFeature extractor saved in wav2vec2-base-finetuned-ks/checkpoint-156/preprocessor_config.json\nDeleting older checkpoint [wav2vec2-base-finetuned-ks/checkpoint-120] due to args.save_total_limit\n***** Running Evaluation *****\n  Num examples = 44\n  Batch size = 8\n","output_type":"stream"},{"name":"stdout","text":"[0 2 0 0 0 0 0 0 0 0 0 1 1 0 0 1 1 1 2 2 3 1 2 2 3 0 1 0 1 3 1 2 2 1 3 3 0\n 0 0 0 3 1 3 1]\n[0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3\n 3 3 3 3 3 3 3]\n{'accuracy': 0.5454545454545454}\n","output_type":"stream"},{"name":"stderr","text":"Saving model checkpoint to wav2vec2-base-finetuned-ks/checkpoint-168\nConfiguration saved in wav2vec2-base-finetuned-ks/checkpoint-168/config.json\nModel weights saved in wav2vec2-base-finetuned-ks/checkpoint-168/pytorch_model.bin\nFeature extractor saved in wav2vec2-base-finetuned-ks/checkpoint-168/preprocessor_config.json\nDeleting older checkpoint [wav2vec2-base-finetuned-ks/checkpoint-132] due to args.save_total_limit\n***** Running Evaluation *****\n  Num examples = 44\n  Batch size = 8\n","output_type":"stream"},{"name":"stdout","text":"[0 2 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 1 2 0 3 1 2 2 3 2 0 0 0 3 1 2 3 3 3 3 0\n 0 0 0 3 0 3 3]\n[0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3\n 3 3 3 3 3 3 3]\n{'accuracy': 0.5681818181818182}\n","output_type":"stream"},{"name":"stderr","text":"Saving model checkpoint to wav2vec2-base-finetuned-ks/checkpoint-180\nConfiguration saved in wav2vec2-base-finetuned-ks/checkpoint-180/config.json\nModel weights saved in wav2vec2-base-finetuned-ks/checkpoint-180/pytorch_model.bin\nFeature extractor saved in wav2vec2-base-finetuned-ks/checkpoint-180/preprocessor_config.json\nDeleting older checkpoint [wav2vec2-base-finetuned-ks/checkpoint-144] due to args.save_total_limit\n***** Running Evaluation *****\n  Num examples = 44\n  Batch size = 8\n","output_type":"stream"},{"name":"stdout","text":"[0 0 0 0 0 0 0 0 0 0 0 1 2 0 0 2 2 1 2 2 3 1 2 2 3 2 2 0 1 3 1 2 3 3 3 3 0\n 3 0 0 3 2 3 3]\n[0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3\n 3 3 3 3 3 3 3]\n{'accuracy': 0.5909090909090909}\n","output_type":"stream"},{"name":"stderr","text":"Saving model checkpoint to wav2vec2-base-finetuned-ks/checkpoint-192\nConfiguration saved in wav2vec2-base-finetuned-ks/checkpoint-192/config.json\nModel weights saved in wav2vec2-base-finetuned-ks/checkpoint-192/pytorch_model.bin\nFeature extractor saved in wav2vec2-base-finetuned-ks/checkpoint-192/preprocessor_config.json\nDeleting older checkpoint [wav2vec2-base-finetuned-ks/checkpoint-168] due to args.save_total_limit\n***** Running Evaluation *****\n  Num examples = 44\n  Batch size = 8\n","output_type":"stream"},{"name":"stdout","text":"[0 0 0 0 0 0 3 0 0 0 0 1 1 0 0 0 1 1 2 2 3 1 2 2 3 3 0 2 1 3 1 2 3 3 3 3 0\n 3 0 2 3 2 3 3]\n[0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3\n 3 3 3 3 3 3 3]\n{'accuracy': 0.5909090909090909}\n","output_type":"stream"},{"name":"stderr","text":"Saving model checkpoint to wav2vec2-base-finetuned-ks/checkpoint-204\nConfiguration saved in wav2vec2-base-finetuned-ks/checkpoint-204/config.json\nModel weights saved in wav2vec2-base-finetuned-ks/checkpoint-204/pytorch_model.bin\nFeature extractor saved in wav2vec2-base-finetuned-ks/checkpoint-204/preprocessor_config.json\nDeleting older checkpoint [wav2vec2-base-finetuned-ks/checkpoint-180] due to args.save_total_limit\n***** Running Evaluation *****\n  Num examples = 44\n  Batch size = 8\n","output_type":"stream"},{"name":"stdout","text":"[0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 1 1 2 2 3 1 0 2 3 0 1 2 1 3 1 2 2 1 3 1 0\n 3 0 2 3 1 3 2]\n[0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3\n 3 3 3 3 3 3 3]\n{'accuracy': 0.5681818181818182}\n","output_type":"stream"},{"name":"stderr","text":"Saving model checkpoint to wav2vec2-base-finetuned-ks/checkpoint-216\nConfiguration saved in wav2vec2-base-finetuned-ks/checkpoint-216/config.json\nModel weights saved in wav2vec2-base-finetuned-ks/checkpoint-216/pytorch_model.bin\nFeature extractor saved in wav2vec2-base-finetuned-ks/checkpoint-216/preprocessor_config.json\nDeleting older checkpoint [wav2vec2-base-finetuned-ks/checkpoint-192] due to args.save_total_limit\n***** Running Evaluation *****\n  Num examples = 44\n  Batch size = 8\n","output_type":"stream"},{"name":"stdout","text":"[0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 2 1 1 2 2 3 1 0 2 3 2 0 2 1 3 1 2 2 3 3 3 0\n 3 0 2 3 1 3 2]\n[0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3\n 3 3 3 3 3 3 3]\n{'accuracy': 0.6136363636363636}\n","output_type":"stream"},{"name":"stderr","text":"Saving model checkpoint to wav2vec2-base-finetuned-ks/checkpoint-228\nConfiguration saved in wav2vec2-base-finetuned-ks/checkpoint-228/config.json\nModel weights saved in wav2vec2-base-finetuned-ks/checkpoint-228/pytorch_model.bin\nFeature extractor saved in wav2vec2-base-finetuned-ks/checkpoint-228/preprocessor_config.json\nDeleting older checkpoint [wav2vec2-base-finetuned-ks/checkpoint-156] due to args.save_total_limit\n***** Running Evaluation *****\n  Num examples = 44\n  Batch size = 8\n","output_type":"stream"},{"name":"stdout","text":"[0 0 0 0 0 0 3 0 0 0 0 1 1 0 3 2 1 1 2 2 3 1 0 2 3 0 0 2 1 3 1 2 2 2 3 3 0\n 3 0 2 3 1 3 2]\n[0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3\n 3 3 3 3 3 3 3]\n{'accuracy': 0.5454545454545454}\n","output_type":"stream"},{"name":"stderr","text":"Saving model checkpoint to wav2vec2-base-finetuned-ks/checkpoint-240\nConfiguration saved in wav2vec2-base-finetuned-ks/checkpoint-240/config.json\nModel weights saved in wav2vec2-base-finetuned-ks/checkpoint-240/pytorch_model.bin\nFeature extractor saved in wav2vec2-base-finetuned-ks/checkpoint-240/preprocessor_config.json\nDeleting older checkpoint [wav2vec2-base-finetuned-ks/checkpoint-204] due to args.save_total_limit\n***** Running Evaluation *****\n  Num examples = 44\n  Batch size = 8\n","output_type":"stream"},{"name":"stdout","text":"[0 0 0 0 0 0 2 0 0 0 0 1 1 3 3 2 1 1 1 2 3 1 2 2 2 2 1 2 1 3 1 2 2 2 2 2 0\n 3 0 2 3 1 3 2]\n[0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3\n 3 3 3 3 3 3 3]\n{'accuracy': 0.5909090909090909}\n","output_type":"stream"},{"name":"stderr","text":"Saving model checkpoint to wav2vec2-base-finetuned-ks/checkpoint-252\nConfiguration saved in wav2vec2-base-finetuned-ks/checkpoint-252/config.json\nModel weights saved in wav2vec2-base-finetuned-ks/checkpoint-252/pytorch_model.bin\nFeature extractor saved in wav2vec2-base-finetuned-ks/checkpoint-252/preprocessor_config.json\nDeleting older checkpoint [wav2vec2-base-finetuned-ks/checkpoint-216] due to args.save_total_limit\n***** Running Evaluation *****\n  Num examples = 44\n  Batch size = 8\n","output_type":"stream"},{"name":"stdout","text":"[0 0 0 0 0 0 2 0 0 0 0 1 1 3 3 2 1 1 2 2 3 1 0 2 3 0 0 2 1 3 1 2 2 2 2 2 0\n 3 0 2 3 1 3 2]\n[0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3\n 3 3 3 3 3 3 3]\n{'accuracy': 0.5}\n","output_type":"stream"},{"name":"stderr","text":"Saving model checkpoint to wav2vec2-base-finetuned-ks/checkpoint-264\nConfiguration saved in wav2vec2-base-finetuned-ks/checkpoint-264/config.json\nModel weights saved in wav2vec2-base-finetuned-ks/checkpoint-264/pytorch_model.bin\nFeature extractor saved in wav2vec2-base-finetuned-ks/checkpoint-264/preprocessor_config.json\nDeleting older checkpoint [wav2vec2-base-finetuned-ks/checkpoint-240] due to args.save_total_limit\n***** Running Evaluation *****\n  Num examples = 44\n  Batch size = 8\n","output_type":"stream"},{"name":"stdout","text":"[0 0 0 0 0 0 3 0 0 0 0 1 1 0 3 0 1 1 1 1 3 1 0 2 3 2 0 2 1 0 1 2 2 2 3 2 0\n 3 0 2 3 1 3 2]\n[0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3\n 3 3 3 3 3 3 3]\n{'accuracy': 0.5909090909090909}\n","output_type":"stream"},{"name":"stderr","text":"Saving model checkpoint to wav2vec2-base-finetuned-ks/checkpoint-276\nConfiguration saved in wav2vec2-base-finetuned-ks/checkpoint-276/config.json\nModel weights saved in wav2vec2-base-finetuned-ks/checkpoint-276/pytorch_model.bin\nFeature extractor saved in wav2vec2-base-finetuned-ks/checkpoint-276/preprocessor_config.json\nDeleting older checkpoint [wav2vec2-base-finetuned-ks/checkpoint-252] due to args.save_total_limit\n***** Running Evaluation *****\n  Num examples = 44\n  Batch size = 8\n","output_type":"stream"},{"name":"stdout","text":"[0 0 0 0 0 0 3 0 0 0 0 1 1 3 3 2 1 1 2 2 3 1 0 2 3 2 0 2 1 0 1 2 2 3 3 3 0\n 3 0 2 3 2 3 2]\n[0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3\n 3 3 3 3 3 3 3]\n{'accuracy': 0.5909090909090909}\n","output_type":"stream"},{"name":"stderr","text":"Saving model checkpoint to wav2vec2-base-finetuned-ks/checkpoint-288\nConfiguration saved in wav2vec2-base-finetuned-ks/checkpoint-288/config.json\nModel weights saved in wav2vec2-base-finetuned-ks/checkpoint-288/pytorch_model.bin\nFeature extractor saved in wav2vec2-base-finetuned-ks/checkpoint-288/preprocessor_config.json\nDeleting older checkpoint [wav2vec2-base-finetuned-ks/checkpoint-264] due to args.save_total_limit\n***** Running Evaluation *****\n  Num examples = 44\n  Batch size = 8\n","output_type":"stream"},{"name":"stdout","text":"[0 0 0 0 0 0 2 0 0 0 0 1 1 3 3 2 1 1 2 2 2 1 0 2 3 2 0 2 2 0 1 2 2 3 2 2 0\n 3 0 2 3 2 3 2]\n[0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3\n 3 3 3 3 3 3 3]\n{'accuracy': 0.5681818181818182}\n","output_type":"stream"},{"name":"stderr","text":"Saving model checkpoint to wav2vec2-base-finetuned-ks/checkpoint-300\nConfiguration saved in wav2vec2-base-finetuned-ks/checkpoint-300/config.json\nModel weights saved in wav2vec2-base-finetuned-ks/checkpoint-300/pytorch_model.bin\nFeature extractor saved in wav2vec2-base-finetuned-ks/checkpoint-300/preprocessor_config.json\nDeleting older checkpoint [wav2vec2-base-finetuned-ks/checkpoint-276] due to args.save_total_limit\n***** Running Evaluation *****\n  Num examples = 44\n  Batch size = 8\n","output_type":"stream"},{"name":"stdout","text":"[0 0 0 0 0 0 2 0 0 0 0 1 1 3 3 2 1 2 0 0 3 1 0 2 3 2 0 2 1 0 1 2 2 3 3 2 0\n 3 0 2 3 2 3 2]\n[0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3\n 3 3 3 3 3 3 3]\n{'accuracy': 0.5454545454545454}\n","output_type":"stream"},{"name":"stderr","text":"Saving model checkpoint to wav2vec2-base-finetuned-ks/checkpoint-312\nConfiguration saved in wav2vec2-base-finetuned-ks/checkpoint-312/config.json\nModel weights saved in wav2vec2-base-finetuned-ks/checkpoint-312/pytorch_model.bin\nFeature extractor saved in wav2vec2-base-finetuned-ks/checkpoint-312/preprocessor_config.json\nDeleting older checkpoint [wav2vec2-base-finetuned-ks/checkpoint-288] due to args.save_total_limit\n***** Running Evaluation *****\n  Num examples = 44\n  Batch size = 8\n","output_type":"stream"},{"name":"stdout","text":"[0 0 0 0 0 0 3 0 0 0 0 1 1 3 3 2 1 1 2 2 3 1 0 2 3 2 2 2 1 0 1 2 2 3 3 3 0\n 3 0 2 3 2 3 2]\n[0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3\n 3 3 3 3 3 3 3]\n{'accuracy': 0.6136363636363636}\n","output_type":"stream"},{"name":"stderr","text":"Saving model checkpoint to wav2vec2-base-finetuned-ks/checkpoint-324\nConfiguration saved in wav2vec2-base-finetuned-ks/checkpoint-324/config.json\nModel weights saved in wav2vec2-base-finetuned-ks/checkpoint-324/pytorch_model.bin\nFeature extractor saved in wav2vec2-base-finetuned-ks/checkpoint-324/preprocessor_config.json\nDeleting older checkpoint [wav2vec2-base-finetuned-ks/checkpoint-300] due to args.save_total_limit\n***** Running Evaluation *****\n  Num examples = 44\n  Batch size = 8\n","output_type":"stream"},{"name":"stdout","text":"[0 0 0 0 0 0 2 0 0 0 0 1 2 3 0 2 1 1 2 2 3 1 0 2 3 2 0 2 1 0 1 2 2 3 2 2 0\n 3 0 2 3 2 3 2]\n[0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3\n 3 3 3 3 3 3 3]\n{'accuracy': 0.5227272727272727}\n","output_type":"stream"},{"name":"stderr","text":"Saving model checkpoint to wav2vec2-base-finetuned-ks/checkpoint-336\nConfiguration saved in wav2vec2-base-finetuned-ks/checkpoint-336/config.json\nModel weights saved in wav2vec2-base-finetuned-ks/checkpoint-336/pytorch_model.bin\nFeature extractor saved in wav2vec2-base-finetuned-ks/checkpoint-336/preprocessor_config.json\nDeleting older checkpoint [wav2vec2-base-finetuned-ks/checkpoint-312] due to args.save_total_limit\n***** Running Evaluation *****\n  Num examples = 44\n  Batch size = 8\n","output_type":"stream"},{"name":"stdout","text":"[0 0 0 0 0 0 2 0 0 0 0 1 1 3 3 2 1 1 2 2 2 1 0 2 2 2 0 2 1 0 1 2 2 3 2 2 0\n 3 0 2 3 2 3 2]\n[0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3\n 3 3 3 3 3 3 3]\n{'accuracy': 0.5681818181818182}\n","output_type":"stream"},{"name":"stderr","text":"Saving model checkpoint to wav2vec2-base-finetuned-ks/checkpoint-348\nConfiguration saved in wav2vec2-base-finetuned-ks/checkpoint-348/config.json\nModel weights saved in wav2vec2-base-finetuned-ks/checkpoint-348/pytorch_model.bin\nFeature extractor saved in wav2vec2-base-finetuned-ks/checkpoint-348/preprocessor_config.json\nDeleting older checkpoint [wav2vec2-base-finetuned-ks/checkpoint-324] due to args.save_total_limit\n***** Running Evaluation *****\n  Num examples = 44\n  Batch size = 8\n","output_type":"stream"},{"name":"stdout","text":"[0 0 0 0 0 0 2 0 0 0 0 1 1 3 3 2 1 1 1 2 3 1 0 2 3 2 1 2 1 0 1 2 2 3 3 2 0\n 3 0 2 3 1 3 2]\n[0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3\n 3 3 3 3 3 3 3]\n{'accuracy': 0.5909090909090909}\n","output_type":"stream"},{"name":"stderr","text":"Saving model checkpoint to wav2vec2-base-finetuned-ks/checkpoint-360\nConfiguration saved in wav2vec2-base-finetuned-ks/checkpoint-360/config.json\nModel weights saved in wav2vec2-base-finetuned-ks/checkpoint-360/pytorch_model.bin\nFeature extractor saved in wav2vec2-base-finetuned-ks/checkpoint-360/preprocessor_config.json\nDeleting older checkpoint [wav2vec2-base-finetuned-ks/checkpoint-336] due to args.save_total_limit\n***** Running Evaluation *****\n  Num examples = 44\n  Batch size = 8\n","output_type":"stream"},{"name":"stdout","text":"[0 0 0 0 0 0 2 0 0 0 0 2 1 3 3 2 2 1 2 2 3 2 0 2 3 2 2 2 1 0 1 2 2 3 3 3 0\n 3 0 2 3 2 3 2]\n[0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3\n 3 3 3 3 3 3 3]\n{'accuracy': 0.5454545454545454}\n","output_type":"stream"},{"name":"stderr","text":"Saving model checkpoint to wav2vec2-base-finetuned-ks/checkpoint-372\nConfiguration saved in wav2vec2-base-finetuned-ks/checkpoint-372/config.json\nModel weights saved in wav2vec2-base-finetuned-ks/checkpoint-372/pytorch_model.bin\nFeature extractor saved in wav2vec2-base-finetuned-ks/checkpoint-372/preprocessor_config.json\nDeleting older checkpoint [wav2vec2-base-finetuned-ks/checkpoint-348] due to args.save_total_limit\n***** Running Evaluation *****\n  Num examples = 44\n  Batch size = 8\n","output_type":"stream"},{"name":"stdout","text":"[0 0 0 0 0 0 2 0 0 0 0 1 1 3 3 2 1 1 2 2 3 1 0 2 3 2 1 2 1 0 1 2 2 3 3 2 0\n 3 0 2 3 1 3 2]\n[0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3\n 3 3 3 3 3 3 3]\n{'accuracy': 0.5681818181818182}\n","output_type":"stream"},{"name":"stderr","text":"Saving model checkpoint to wav2vec2-base-finetuned-ks/checkpoint-384\nConfiguration saved in wav2vec2-base-finetuned-ks/checkpoint-384/config.json\nModel weights saved in wav2vec2-base-finetuned-ks/checkpoint-384/pytorch_model.bin\nFeature extractor saved in wav2vec2-base-finetuned-ks/checkpoint-384/preprocessor_config.json\nDeleting older checkpoint [wav2vec2-base-finetuned-ks/checkpoint-360] due to args.save_total_limit\n***** Running Evaluation *****\n  Num examples = 44\n  Batch size = 8\n","output_type":"stream"},{"name":"stdout","text":"[0 0 0 0 0 0 2 0 0 0 0 1 1 3 3 2 1 1 2 2 2 1 0 2 3 2 2 2 1 0 1 2 2 3 3 2 0\n 2 0 2 3 1 3 2]\n[0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3\n 3 3 3 3 3 3 3]\n{'accuracy': 0.5681818181818182}\n","output_type":"stream"},{"name":"stderr","text":"Saving model checkpoint to wav2vec2-base-finetuned-ks/checkpoint-396\nConfiguration saved in wav2vec2-base-finetuned-ks/checkpoint-396/config.json\nModel weights saved in wav2vec2-base-finetuned-ks/checkpoint-396/pytorch_model.bin\nFeature extractor saved in wav2vec2-base-finetuned-ks/checkpoint-396/preprocessor_config.json\nDeleting older checkpoint [wav2vec2-base-finetuned-ks/checkpoint-372] due to args.save_total_limit\n***** Running Evaluation *****\n  Num examples = 44\n  Batch size = 8\n","output_type":"stream"},{"name":"stdout","text":"[0 0 0 0 0 0 2 0 0 0 0 1 1 3 3 2 1 1 2 2 2 1 0 2 2 2 2 2 1 0 1 2 2 3 3 2 0\n 2 0 2 3 2 3 2]\n[0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3\n 3 3 3 3 3 3 3]\n{'accuracy': 0.5909090909090909}\n","output_type":"stream"},{"name":"stderr","text":"Saving model checkpoint to wav2vec2-base-finetuned-ks/checkpoint-408\nConfiguration saved in wav2vec2-base-finetuned-ks/checkpoint-408/config.json\nModel weights saved in wav2vec2-base-finetuned-ks/checkpoint-408/pytorch_model.bin\nFeature extractor saved in wav2vec2-base-finetuned-ks/checkpoint-408/preprocessor_config.json\nDeleting older checkpoint [wav2vec2-base-finetuned-ks/checkpoint-384] due to args.save_total_limit\n***** Running Evaluation *****\n  Num examples = 44\n  Batch size = 8\n","output_type":"stream"},{"name":"stdout","text":"[0 0 0 0 0 0 2 0 0 0 0 1 1 3 3 2 1 1 2 2 2 1 0 2 2 2 2 2 1 0 1 2 2 3 3 2 0\n 3 0 2 3 2 3 2]\n[0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3\n 3 3 3 3 3 3 3]\n{'accuracy': 0.6136363636363636}\n","output_type":"stream"},{"name":"stderr","text":"Saving model checkpoint to wav2vec2-base-finetuned-ks/checkpoint-420\nConfiguration saved in wav2vec2-base-finetuned-ks/checkpoint-420/config.json\nModel weights saved in wav2vec2-base-finetuned-ks/checkpoint-420/pytorch_model.bin\nFeature extractor saved in wav2vec2-base-finetuned-ks/checkpoint-420/preprocessor_config.json\nDeleting older checkpoint [wav2vec2-base-finetuned-ks/checkpoint-396] due to args.save_total_limit\n***** Running Evaluation *****\n  Num examples = 44\n  Batch size = 8\n","output_type":"stream"},{"name":"stdout","text":"[0 0 0 0 0 0 2 0 0 0 0 1 1 2 3 2 1 1 1 2 1 1 0 2 2 2 2 2 1 0 1 2 2 3 2 2 0\n 2 0 2 3 1 3 2]\n[0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3\n 3 3 3 3 3 3 3]\n{'accuracy': 0.6136363636363636}\n","output_type":"stream"},{"name":"stderr","text":"Saving model checkpoint to wav2vec2-base-finetuned-ks/checkpoint-432\nConfiguration saved in wav2vec2-base-finetuned-ks/checkpoint-432/config.json\nModel weights saved in wav2vec2-base-finetuned-ks/checkpoint-432/pytorch_model.bin\nFeature extractor saved in wav2vec2-base-finetuned-ks/checkpoint-432/preprocessor_config.json\nDeleting older checkpoint [wav2vec2-base-finetuned-ks/checkpoint-408] due to args.save_total_limit\n***** Running Evaluation *****\n  Num examples = 44\n  Batch size = 8\n","output_type":"stream"},{"name":"stdout","text":"[0 0 0 0 0 0 2 0 0 0 0 1 1 3 3 2 1 1 1 2 1 1 0 2 2 2 2 2 1 0 1 2 2 3 3 2 0\n 2 0 2 3 1 3 2]\n[0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3\n 3 3 3 3 3 3 3]\n{'accuracy': 0.6363636363636364}\n","output_type":"stream"},{"name":"stderr","text":"Saving model checkpoint to wav2vec2-base-finetuned-ks/checkpoint-444\nConfiguration saved in wav2vec2-base-finetuned-ks/checkpoint-444/config.json\nModel weights saved in wav2vec2-base-finetuned-ks/checkpoint-444/pytorch_model.bin\nFeature extractor saved in wav2vec2-base-finetuned-ks/checkpoint-444/preprocessor_config.json\nDeleting older checkpoint [wav2vec2-base-finetuned-ks/checkpoint-228] due to args.save_total_limit\n***** Running Evaluation *****\n  Num examples = 44\n  Batch size = 8\n","output_type":"stream"},{"name":"stdout","text":"[0 0 0 0 0 0 2 0 0 0 0 1 1 3 3 2 1 1 2 2 3 1 0 2 2 2 2 2 1 0 1 2 2 3 3 2 0\n 2 0 2 3 1 3 2]\n[0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3\n 3 3 3 3 3 3 3]\n{'accuracy': 0.5909090909090909}\n","output_type":"stream"},{"name":"stderr","text":"Saving model checkpoint to wav2vec2-base-finetuned-ks/checkpoint-456\nConfiguration saved in wav2vec2-base-finetuned-ks/checkpoint-456/config.json\nModel weights saved in wav2vec2-base-finetuned-ks/checkpoint-456/pytorch_model.bin\nFeature extractor saved in wav2vec2-base-finetuned-ks/checkpoint-456/preprocessor_config.json\nDeleting older checkpoint [wav2vec2-base-finetuned-ks/checkpoint-420] due to args.save_total_limit\n***** Running Evaluation *****\n  Num examples = 44\n  Batch size = 8\n","output_type":"stream"},{"name":"stdout","text":"[0 0 0 0 0 0 2 0 0 0 0 1 1 3 3 2 1 1 2 2 2 1 0 2 2 2 2 2 1 0 1 2 2 3 3 2 0\n 2 0 2 3 1 3 2]\n[0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3\n 3 3 3 3 3 3 3]\n{'accuracy': 0.5909090909090909}\n","output_type":"stream"},{"name":"stderr","text":"Saving model checkpoint to wav2vec2-base-finetuned-ks/checkpoint-468\nConfiguration saved in wav2vec2-base-finetuned-ks/checkpoint-468/config.json\nModel weights saved in wav2vec2-base-finetuned-ks/checkpoint-468/pytorch_model.bin\nFeature extractor saved in wav2vec2-base-finetuned-ks/checkpoint-468/preprocessor_config.json\nDeleting older checkpoint [wav2vec2-base-finetuned-ks/checkpoint-432] due to args.save_total_limit\n***** Running Evaluation *****\n  Num examples = 44\n  Batch size = 8\n","output_type":"stream"},{"name":"stdout","text":"[0 0 0 0 0 0 2 0 0 0 0 1 1 2 3 2 1 1 1 2 1 1 0 2 2 2 2 2 1 0 1 2 2 3 3 2 0\n 2 0 2 3 1 3 2]\n[0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3\n 3 3 3 3 3 3 3]\n{'accuracy': 0.6363636363636364}\n","output_type":"stream"},{"name":"stderr","text":"Saving model checkpoint to wav2vec2-base-finetuned-ks/checkpoint-480\nConfiguration saved in wav2vec2-base-finetuned-ks/checkpoint-480/config.json\nModel weights saved in wav2vec2-base-finetuned-ks/checkpoint-480/pytorch_model.bin\nFeature extractor saved in wav2vec2-base-finetuned-ks/checkpoint-480/preprocessor_config.json\nDeleting older checkpoint [wav2vec2-base-finetuned-ks/checkpoint-456] due to args.save_total_limit\n***** Running Evaluation *****\n  Num examples = 44\n  Batch size = 8\n","output_type":"stream"},{"name":"stdout","text":"[0 0 0 0 0 0 2 0 0 0 0 1 1 3 3 2 1 1 1 2 2 1 0 2 2 2 2 2 1 0 1 2 2 3 3 2 0\n 2 0 2 3 1 3 2]\n[0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3\n 3 3 3 3 3 3 3]\n{'accuracy': 0.6136363636363636}\n","output_type":"stream"},{"name":"stderr","text":"Saving model checkpoint to wav2vec2-base-finetuned-ks/checkpoint-492\nConfiguration saved in wav2vec2-base-finetuned-ks/checkpoint-492/config.json\nModel weights saved in wav2vec2-base-finetuned-ks/checkpoint-492/pytorch_model.bin\nFeature extractor saved in wav2vec2-base-finetuned-ks/checkpoint-492/preprocessor_config.json\nDeleting older checkpoint [wav2vec2-base-finetuned-ks/checkpoint-468] due to args.save_total_limit\n***** Running Evaluation *****\n  Num examples = 44\n  Batch size = 8\n","output_type":"stream"},{"name":"stdout","text":"[0 0 0 0 0 0 2 0 0 0 0 1 1 2 3 2 1 1 1 2 1 1 0 2 2 2 2 2 1 0 1 2 2 3 3 2 0\n 2 0 2 3 1 3 2]\n[0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3\n 3 3 3 3 3 3 3]\n{'accuracy': 0.6363636363636364}\n","output_type":"stream"},{"name":"stderr","text":"Saving model checkpoint to wav2vec2-base-finetuned-ks/checkpoint-504\nConfiguration saved in wav2vec2-base-finetuned-ks/checkpoint-504/config.json\nModel weights saved in wav2vec2-base-finetuned-ks/checkpoint-504/pytorch_model.bin\nFeature extractor saved in wav2vec2-base-finetuned-ks/checkpoint-504/preprocessor_config.json\nDeleting older checkpoint [wav2vec2-base-finetuned-ks/checkpoint-480] due to args.save_total_limit\n***** Running Evaluation *****\n  Num examples = 44\n  Batch size = 8\n","output_type":"stream"},{"name":"stdout","text":"[0 0 0 0 0 0 2 0 0 0 0 1 1 2 3 2 1 1 2 2 1 1 0 2 2 2 2 2 1 0 1 2 2 3 3 2 0\n 2 0 2 3 1 3 2]\n[0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3\n 3 3 3 3 3 3 3]\n{'accuracy': 0.6136363636363636}\n","output_type":"stream"},{"name":"stderr","text":"Saving model checkpoint to wav2vec2-base-finetuned-ks/checkpoint-516\nConfiguration saved in wav2vec2-base-finetuned-ks/checkpoint-516/config.json\nModel weights saved in wav2vec2-base-finetuned-ks/checkpoint-516/pytorch_model.bin\nFeature extractor saved in wav2vec2-base-finetuned-ks/checkpoint-516/preprocessor_config.json\nDeleting older checkpoint [wav2vec2-base-finetuned-ks/checkpoint-492] due to args.save_total_limit\n***** Running Evaluation *****\n  Num examples = 44\n  Batch size = 8\n","output_type":"stream"},{"name":"stdout","text":"[0 0 0 0 0 0 2 0 0 0 0 1 1 3 3 2 1 1 2 2 2 1 0 2 2 2 2 2 1 0 1 2 2 3 3 2 0\n 2 0 2 3 1 3 2]\n[0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3\n 3 3 3 3 3 3 3]\n{'accuracy': 0.5909090909090909}\n","output_type":"stream"},{"name":"stderr","text":"Saving model checkpoint to wav2vec2-base-finetuned-ks/checkpoint-528\nConfiguration saved in wav2vec2-base-finetuned-ks/checkpoint-528/config.json\nModel weights saved in wav2vec2-base-finetuned-ks/checkpoint-528/pytorch_model.bin\nFeature extractor saved in wav2vec2-base-finetuned-ks/checkpoint-528/preprocessor_config.json\nDeleting older checkpoint [wav2vec2-base-finetuned-ks/checkpoint-504] due to args.save_total_limit\n***** Running Evaluation *****\n  Num examples = 44\n  Batch size = 8\n","output_type":"stream"},{"name":"stdout","text":"[0 0 0 0 0 0 2 0 0 0 0 1 1 3 3 2 1 1 2 2 1 1 0 2 2 2 2 2 1 0 1 2 2 3 3 2 0\n 2 0 2 3 1 3 2]\n[0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3\n 3 3 3 3 3 3 3]\n{'accuracy': 0.6136363636363636}\n","output_type":"stream"},{"name":"stderr","text":"Saving model checkpoint to wav2vec2-base-finetuned-ks/checkpoint-540\nConfiguration saved in wav2vec2-base-finetuned-ks/checkpoint-540/config.json\nModel weights saved in wav2vec2-base-finetuned-ks/checkpoint-540/pytorch_model.bin\nFeature extractor saved in wav2vec2-base-finetuned-ks/checkpoint-540/preprocessor_config.json\nDeleting older checkpoint [wav2vec2-base-finetuned-ks/checkpoint-516] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from wav2vec2-base-finetuned-ks/checkpoint-444 (score: 0.6363636363636364).\n","output_type":"stream"},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=540, training_loss=0.6825715566398921, metrics={'train_runtime': 10773.6493, 'train_samples_per_second': 1.637, 'train_steps_per_second': 0.05, 'total_flos': 4.80225962225664e+18, 'train_loss': 0.6825715566398921, 'epoch': 44.98})"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}