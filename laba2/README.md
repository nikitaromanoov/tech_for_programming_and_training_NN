# Лабораторная работа №2 - Отчет
## Теоретическая часть
Классификация звука - присваивает метку класса аудио на основе входных данных - необработанных звуковых сигналов.
Некоторые практические применения классификации звуков включают определение намерений говорящего, языковую классификацию и даже виды животных по их звукам.

Facebook's Wav2Vec2 - базовая модель, которая предварительно обучалась на сэмплированном звуке речи частотой 16 кГц. 
Она маскирует скрытые представления необработанной формы сигнала и решает контрастную задачу по сравнению с квантованными речевыми
представлениями. Эксперименты показывают большой потенциал предварительной подготовки на немаркированных данных для
обработки речи. Модель достигает результатов, которые соответствуют новому уровню техники в полном тесте Librispeech для
шумной речи. При чистой 100-часовой настройке Librispeech wav2vec 2.0 превосходит предыдущий лучший
результат, используя в 100 раз меньше помеченных данных. Этот подход также эффективен при наличии больших объемов
помеченных данных. Также имеется повышение производительности за счет перехода на архитектуру seq2seq и
словарный запас word piece. [1]

Архитектура окончательной модели, используемой для прогнозирования, состоит из трех основных частей: [2]
1. Cверточные слои, которые обрабатывают исходную форму сигнала для получения скрытого представления - Z
2. Cлои-трансформеры, создающие контекстуализированное представление - C
3. Линейная проекция на вывод - Y

![image](https://user-images.githubusercontent.com/91135334/209224611-72f6f8be-c4c4-4e69-bb2c-bc41a14c334f.png)

Основная идея предварительной подготовки аналогична BERT: часть входных данных преобразователя замаскирована, и цель состоит в том, чтобы угадать замаскированное представление вектора скрытых признаков Zₜ. Однако авторы усовершенствовали эту простую идею с помощью контрастивного обучения.

## Задание 1
### Задача
Согласно варианту выбрать предварительно обученную  модель и на основе Audio Embeddings (Wav2Vec2) датасета Youtube Videos Dataset дообучить ее для задачи классификации. Оценить качество модели.

### Код
`lab2-part1.ipynb` - data downloading

`lab2-part2.ipynb` - fine-tunning model
### Разработанная система

C помощью кода lab2-part1.ipynb была выкачана половина датасета [3],  был выгружен не весь датасет, поскольку это слишком долго, так как имеются видео, длящиеся более 11 часов. Среди этой половины датасета, не все данные получилось прочитать, так как многие видео уже недоступны. Также данные сильно несбалансированные, так было получено:
* history - 109 
* art_music - 109
* travel -181
* food - 145

Было решено просто взять по 109 семплов из каждого класса, 11 на valid и 98 на train

![загруженное (1)](https://user-images.githubusercontent.com/91135334/209225920-03c13b0f-98d5-49e9-952a-637819d94231.png)

После того как были загружены данные, скачанные в файле lab2-part1.ipynb, был начат препроцессинг данных и дальнейшая тонкая настройка. 
Для  разработки использовались в основном такие библиотеки, как - transformers, datasets, pandas, numpy.

Был загружен Wav2Vec2 feature extractor для обработки аудиосигнала. Была выбрана именно эта модель по той причине, что она дает отличную производительность без больших объемов данных. Далее была создана функция предварительной обработки.

Затем была загружена модель facebook/wav2vec2-base на 4 класса и прописаны параметры для дальнейшего обучения. 


### Результаты

Результаты обучения представлены ниже:
![Без имени](https://user-images.githubusercontent.com/91135334/209283377-ab010beb-44a0-444f-9342-38e3312a4f89.png)
Обучение было закончено, так как на протяжении большого количества эпох значение метрик на  valid перестало улучшаться 



### Выводы

Таким образом, была обучена модель для классификации видео из  YouTube на основе аудиоэмбендингов, метрика  accuracy показала наилучшее значение - 63.64%, что лучше наивного алгоритма (25%), который выбирал бы самый частый класс на 38.64%.
Можно добиться лучшего результата, если увеличить набор данных или попробовать модели с большим количеством параметров или другими гиперпараметрами. 

## Использованные источники

[1] https://arxiv.org/pdf/2006.11477.pdf

[2] https://towardsdatascience.com/wav2vec-2-0-a-framework-for-self-supervised-learning-of-speech-representations-7d3728688cae

[3] https://www.kaggle.com/datasets/rajatrc1705/youtube-videos-dataset


## Задание 2
### Задача
Реализовать кастомную глубокую нейронную сеть, которая будет на базе аудиоэмбендингов предсказывать класс видео. Сравнить результаты с заданием 1. 

### Код
`lab2-part3.ipynb` - training custom model

### Разработанная система
В ходе выполнения 2 задания были прочтены те же данные, что и для задания 1 для того, чтобы сравнить модели в обоих заданиях. 
Был написан класс CustomdDS, который считывает данные, приводит их в соответствии с заданным sample rate, длиной и вычисляет спектрограмму.
Также была реализована кастомная модель - AudioClassifier для классификации аудио на 4 класса. В завершении было проведено обучение до тех пор, пока значение  val_loss не перестало уменьшаться в течение нескольких эпох, что свидетельствует о необходимости завершения обучения для избежания переобучения.

### Результаты
Результаты обучения представлены ниже:
![image](https://user-images.githubusercontent.com/91135334/209675317-852a1f4c-0b04-4e40-913e-c06036032c69.png)
Заметим, что обучение было остановлено на 15 эпохе, когда все метрики по валиду перестали меняться. Наилучшее значение accuracy_valid равняется 0.49.

### Выводы
Таким образом, можно заметить, что кастомная модель показала себя значительно хуже, чем wav2vec. Во-первых, по наилучшему значению метрики accuracy_valid - отставание на 0.10 на этом же периоде обучения ( 15 эпох) или глобально на 0.146 (wav2vec лучший - 0.636364). Во-вторых, wav2vec раньше выдавало наилучший результат, чем кастомная модель. 

