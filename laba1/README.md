# Лабораторная работа №1 - Отчет
## Теоретическая часть
![image](https://user-images.githubusercontent.com/91135334/207711733-b1e9aa84-0165-4fc4-b0eb-4d41dc5594b7.png)

### Архитектура

VGG ( Visual Geometry Group) -  это архитектура глубокой сверточной нейронной сети (CNN) с несколькими слоями. Популярны такие ее виды, как VGG-16 или VGG-19, состоящие из 16 и 19 соответственно сверточных слоев. Модель VGG предложенна А. Зиссерманом и К. Симоняном из Оксфордского университета. Эти исследователи опубликовали свою модель в исследовательской статье под названием “Очень глубокие сверточные сети для крупномасштабного распознавания изображений”. [4]

Сеть VGG построена с использованием очень маленьких сверточных фильтров. VGG-16 состоит из 13 сверточных слоев и трех полностью соединенных слоев. VGG принимает размер входного изображения 224×224. Сверточные слои VGG используют минимальное поле восприятия, т.е. 3 × 3, наименьший возможный размер, который все еще захватывает вверх / вниз и влево / вправо. Кроме того, существуют также фильтры свертки 1 × 1, действующие  в качестве линейного преобразования входных данных. За этим следует модуль ReLU, который является огромной инновацией от AlexNet, сокращающей время обучения. ReLU – это  функция активации линейного блока; это кусочно-линейная функция, которая выводит входные данные, если они положительны; в противном случае выходные данные равны нулю. Шаг свертки фиксируется на 1 пиксель, чтобы сохранить пространственное разрешение после свертки (шаг - это количество сдвигов пикселей по входной матрице).  VGGNet имеет три полностью подключенных слоя. Из трех уровней первые два имеют 4096 каналов каждый, а третий имеет 1000 каналов, по 1 для каждого класса.

Число 16 в названии VGG относится к тому факту, что это 16-уровневая глубокая нейронная сеть (VGGNet). Это означает, что VGG16 - довольно обширная сеть, имеющая в общей сложности около 138 миллионов параметров. Даже по современным стандартам это огромная сеть. Однако простота архитектуры VGGNet16 — это то, что делает сеть более привлекательной. Просто взглянув на его архитектуру, можно сказать, что она довольно однородна.

Количество фильтров, которые мы можем использовать, удваивается на каждом шаге или через каждый стек слоя свертки. Это основной принцип, используемый при проектировании архитектуры сети VGG16. Одним из важнейших недостатков сети VGG16 является то, что это огромная сеть, а это значит, что на обучение ее параметров требуется больше времени. Благодаря своей глубине и количеству полностью подключенных слоев модель VGG16 имеет размер более 533 МБ. Это делает внедрение сети VGG трудоемкой задачей.

### Оптимизаторы

Алгоритм Адама работает быстрее, чем SGD, но он имеет два основных недостатка: результат может не сходиться и глобальное оптимальное решение может быть не найдено. Другими словами, его способность к обобщению невысока, и его производительность при решении определенных проблем уступает SGD. Причина этих двух дефектов может быть связана с нестабильностью и высокой скоростью обучения. 

Поэтому был разработан новый оптимизатор -  AdaBound, который обеспечивает постепенный переход от адапативного метода к  SGD. Такой оптимизатор позволяет устранить обучающий разрыв между адаптивным методом и SGD, и в то же время поддерживает высокую скорость обучения на ранних этапах обучения, а также обеспечивать сильные возможности обобщения тестовых данных, к тому же он может значительно улучшить прототипы, особенно в сложных глубоких сетях. 

## Задание 1

### Задача
Реализовать c использованием NumPy нейронную сеть с архитектурой VGG и оптимизатором AdaBound и обучить на наборе данных MNIST.

### Код
`lab1_part1` - файл с кодом
### Разработанная система
В ходе выполнения первого задания была написана реализация оптимизатора AdaBound согласно алгоритму, описанному в научной работе [1]. В качестве основы использовался код, написанный с помощью PyTorch[2] и код оптимизатора Adam на языке Python[3].  

Также на языке python с использованием библиотеки NumPy были реализованы следующие функции:
* ReLU – функция активации
* ReLUDerivative – производная функции активации ReLU для использования в обратном распространении
* zero_pad – функция для заполнения нулями
* softmax – функция активации
* sigmoid – функция активации
* conv_single_step – шаг свертки
* create_mask_from_window - функция создает матрицу "маски", которая отслеживает, где находится максимум матрицы.
* distribute_value – функция, распределяющая входное значение в матрице размерной формы
* pool_forward - прямой проход объединяющего слоя
* pool_backward - обратный проход объединяющего слоя

Классы:
* Conv2d– создание сверхточного слоя, используемого в сверхточной нейронной сети (CNN)
* Linear – реализация L-уровня
* VGG16 – реализация архитектуры VGG16 на  NumPy

Обучение проходило на малой выборке данных MNIST, поскольку обучение нейронной сети, написанной на NumPy занимает очень большое время. По этой же причине замер результатов проходил на той же самой выборке, а не на отдельной валидационной, поскольку имеющегося набора данных представленных для обучения в приемлемое время недостаточно для хорошего обобщения данных. 


### Результаты
Ниже представлены графики, полученные при обучении модели. 
![image](https://user-images.githubusercontent.com/91135334/207713125-e07e64d5-38e5-41b9-b1c7-877421fa4f26.png)

### Выводы
Из графиков видно, что модель за несколько эпох обучения может научиться предсказывать класс на данных, что говорит о том, что архитектура VGG и оптимизатор   Adabound были реализованы верно. 

Также хочется отметить, что обучение нейронной сети на библиотеки  NumPy с использованием процессора - очень долгая задача.

## Задание 2. 

### Задача
Реализовать с помощью PyTorch архитектуру нейронной сети -  VGG, а также два оптимизатора -  Adam и  AdaBound и провести их сравнение.
### Код
`lab1_part2` - файл с кодом

### Разработанная система
В ходе выполнения второго задания была написана реализация оптимизаторов  Adam [5] и  AdaBound [1]. Также  была реализована нейронная сеть с использованием библиотеки PyTorch, для этого использовались следующие классы:

* _ConvNd
* Conv2d - применяет 2D-свертку к входному сигналу, состоящему из нескольких входных плоскостей.
* Linear - применяет линейное преобразование к входящим данным
* _Loss - родительский класс для _WeightedLoss
* _WeightedLoss - родительский класс для CrossEntropyLoss
* CrossEntropyLoss - вычисляет потери перекрестной энтропии между входными логитами
и цель
* _MaxPoolNd - родительский класс для MaxPool2d
* MaxPool2d - применяет объединение 2D max к входному сигналу, состоящему из нескольких входных
самолеты.
* ReLU - применяет функцию выпрямленного линейного блока поэлементно
* VGG16 - реализация архитектуры VGG16 на  PyTorch

Обучение проходило на датасете CarDatasets, анализ показал, что в данных содержится 8144 обучающих образца, которые были поделены в примерном сооношении 1:10 на валидационную и обучающие выборки таким образом, что в обеих выборки находились представители каждому класса в нужном количестве. Так как анализ данных показал, что представителей каждому класса примерное одинаковыое колчиество ( от 24 до ~70), то для валидации было взято по 4 случайных представителя каждого класса. Таким образом получилось следующее деление на valid и  train выборку: 784 и  7360 соотвественно. 

Обучение было выполнено 2 раза: 
* с использованием оптимизтора  Adam
* с использовнием оптимизатора  AdaBound

### Результаты
Ниже представлены сравнительные графики, полученные при двух процессах обучения модели. 
![image](https://user-images.githubusercontent.com/91135334/207783175-08bc2d32-535b-4377-8381-0f3de8d6b9c7.png)

Обучение длилось только 100 эпох, так как обучение на архитектуре  VGG занимает большое время, в идеале нужно было обучить еще 50-100 жпох для того момента, как  valid_loss перестанет меняться в течение нескольких эпох. 

### Выводы
Исходя из полученных выше результатов видно, что при использовании оптимизатора  AdaBound, мы получаем болеее хорошие результаты на метриках f1_valid, valid_acc и valid_loss, чем при использовании оптимизатора Adam. Наиболее наглядно это демонстрирует график valid_loss/epoch, а также данные о максимумах и минимумам в легенде к каждому графику.

Таким образом, практические результаты действительно подверждают тот факт, что AdaBound  дает более хорошие результаты за счет того, что его скорость обучения сравнима с Adam, а производительность сравнима с  SGD. 

## Использованные источники

[1] https://arxiv.org/abs/1902.09843v1

[2] https://github.com/Luolc/AdaBound

[3] https://habr.com/ru/company/skillfactory/blog/525214/

[4] https://arxiv.org/pdf/1409.1556.pdf

[5] https://arxiv.org/abs/1412.6980
