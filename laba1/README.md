# Лабораторная работа №1 - Отчет
## Теоретическая часть
![image](https://user-images.githubusercontent.com/91135334/207711733-b1e9aa84-0165-4fc4-b0eb-4d41dc5594b7.png)

VGG ( Visual Geometry Group) -  это архитектура глубокой сверточной нейронной сети (CNN) с несколькими слоями. Популярны такие ее виды, как VGG-16 или VGG-19, состоящие из 16 и 19 соответственно сверточных слоев. Модель VGG предложенна А. Зиссерманом и К. Симоняном из Оксфордского университета. Эти исследователи опубликовали свою модель в исследовательской статье под названием “Очень глубокие сверточные сети для крупномасштабного распознавания изображений”. [4]

Сеть VGG построена с использованием очень маленьких сверточных фильтров. VGG-16 состоит из 13 сверточных слоев и трех полностью соединенных слоев. VGG принимает размер входного изображения 224×224. Сверточные слои VGG используют минимальное поле восприятия, т.е. 3 × 3, наименьший возможный размер, который все еще захватывает вверх / вниз и влево / вправо. Кроме того, существуют также фильтры свертки 1 × 1, действующие  в качестве линейного преобразования входных данных. За этим следует модуль ReLU, который является огромной инновацией от AlexNet, сокращающей время обучения. ReLU – это  функция активации линейного блока; это кусочно-линейная функция, которая выводит входные данные, если они положительны; в противном случае выходные данные равны нулю. Шаг свертки фиксируется на 1 пиксель, чтобы сохранить пространственное разрешение после свертки (шаг - это количество сдвигов пикселей по входной матрице).  VGGNet имеет три полностью подключенных слоя. Из трех уровней первые два имеют 4096 каналов каждый, а третий имеет 1000 каналов, по 1 для каждого класса.

Число 16 в названии VGG относится к тому факту, что это 16-уровневая глубокая нейронная сеть (VGGNet). Это означает, что VGG16 - довольно обширная сеть, имеющая в общей сложности около 138 миллионов параметров. Даже по современным стандартам это огромная сеть. Однако простота архитектуры VGGNet16 — это то, что делает сеть более привлекательной. Просто взглянув на его архитектуру, можно сказать, что она довольно однородна.

Количество фильтров, которые мы можем использовать, удваивается на каждом шаге или через каждый стек слоя свертки. Это основной принцип, используемый при проектировании архитектуры сети VGG16. Одним из важнейших недостатков сети VGG16 является то, что это огромная сеть, а это значит, что на обучение ее параметров требуется больше времени. Благодаря своей глубине и количеству полностью подключенных слоев модель VGG16 имеет размер более 533 МБ. Это делает внедрение сети VGG трудоемкой задачей.

## Задание 1

### Задача
Реализовать c использованием NumPy нейронную сеть с архитектурой VGG и оптимизатором AdaBound и обучить на наборе данных MNIST.
### Разработанная система
В ходе выполнения первого задания была написана реализация оптимизатора AdaBound согласно алгоритму, описанному в научной работе [1]. В качестве основы использовался код, написанный с помощью PyTorch[2] и код оптимизатора Adam на языке Python[3].  

Также на языке python с использованием библиотеки NumPy были реализованы следующие функции:
* ReLU – функция активации
* ReLUDerivative – производная функции активации Relu для использования в обратном распространении
* zero_pad – функция для заполнения нулями
* softmax – функция активации
* sigmoid – функция активации
* conv_single_step – шаг свертки
* create_mask_from_window - функция создает матрицу "маски", которая отслеживает, где находится максимум матрицы.
* distribute_value – функция, распределяющая входное значение в матрице размерной формы
* pool_forward - прямой проход объединяющего слоя
* pool_backward - обратный проход объединяющего слоя
* 
Классы:
* Conv2d– создания сверхточного слоя, используемого в сверхточной нейронной сети (CNN)
* Linear – реализация L-уровня
* VGG16 – реализация архитектуры VGG16
Обучение проходило на малой выборке данных, поскольку обучение нейронной сети, написанной на NumPy занимает очень большое время. По этой же причине замер результатов проходил на той же самой выборке, а не на отдельной валидационной, поскольку имеющегося набора данных представленных для обучения в приемлемое время недостаточно для хорошего обобщения данных. 


### Результаты
Ниже представлены графики, полученные при обучении модели. 
![image](https://user-images.githubusercontent.com/91135334/207713125-e07e64d5-38e5-41b9-b1c7-877421fa4f26.png)

### Выводы
Из графиков видно, что модель за несколько эпох обучения может научиться предсказывать класс на данных, что говорит о том, что архитектура VGG и оптимизатор   Adabound были реализованы верно. 

Также хочется отметить, что обучение нейронной сети на библиотеки  NumPy с использованием процессора - очень долгая задача.

## Задание 2. 

### Задача
Реализовать с помощью PyTorch архитектуру нейронной сети -  VGG, а также два оптимизатора -  Adam и  AdaBound и провести их сравнение.
### Разработанная система

### Результаты

### Выводы


## Использованные источники

[1] https://arxiv.org/abs/1902.09843v1

[2] https://github.com/Luolc/AdaBound

[3] https://habr.com/ru/company/skillfactory/blog/525214/

[4] https://arxiv.org/pdf/1409.1556.pdf
